{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 資料前處理\n",
    "## (a)讀取csv檔僅保留\"text\"、\"stars\"兩個欄位\n",
    "df = pd.read_csv('./archive/yelp.csv')\n",
    "cols = ['text','stars']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "0     My wife took me here on my birthday for breakf...      1\n",
       "1     I have no idea why some people give bad review...      1\n",
       "2     love the gyro plate. Rice is so good and I als...      1\n",
       "3     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      1\n",
       "4     General Manager Scott Petello is a good egg!!!...      1\n",
       "...                                                 ...    ...\n",
       "9995  First visit...Had lunch here today - used my G...      0\n",
       "9996  Should be called house of deliciousness!\\n\\nI ...      1\n",
       "9997  I recently visited Olive and Ivy for business ...      1\n",
       "9998  My nephew just moved to Scottsdale recently so...      0\n",
       "9999  4-5 locations.. all 4.5 star average.. I think...      1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 將stars欄位內值大於等於4的轉成1，其餘轉成0\n",
    "df['stars'] = [0 if star < 4 else 1 for star in df['stars']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "value_list = [row[0] for row in df.itertuples(index=False, name=None)]\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(value_list)\n",
    "X_train.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料前處理 \n",
    "# 1.將text欄位內的文字利用分割符號切割\n",
    "# 2.去除停頓詞 stop_words\n",
    "# 3.更新成詞幹\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "def clean(text):\n",
    "    lemmas_sent = []\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words(\n",
    "        'english')  + ['super', 'duper', 'place']  # 自訂 word2Vec\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lower = [word.lower() for word in tokens]\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    taged_sent = nltk.pos_tag(no_alpha)\n",
    "    for tag in taged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wn.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    clean_text = lemmas_sent    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\AppData\\Local\\Temp/ipykernel_16972/2470327160.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['text'][i] = ' '.join(clean(line))\n"
     ]
    }
   ],
   "source": [
    "df_clean = df\n",
    "for i, line in enumerate(df['text']):\n",
    "    df_clean['text'][i] = ' '.join(clean(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  stars\n",
      "0     wife take birthday breakfast excellent weather...      1\n",
      "1     idea people give bad review go show please eve...      1\n",
      "2     love gyro plate rice good also dig candy selec...      1\n",
      "3     rosie dakota love chaparral dog park convenien...      1\n",
      "4     general manager scott petello good egg go deta...      1\n",
      "...                                                 ...    ...\n",
      "9995  first visit lunch today use groupon order brus...      0\n",
      "9996  call house deliciousness could go item item bl...      1\n",
      "9997  recently visit olive ivy business last week vi...      1\n",
      "9998  nephew move scottsdale recently bunch friend b...      0\n",
      "9999  location star average think arizona really fan...      1\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 文字探勘前處理，將文字轉換成向量 : tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def vectorize(data, tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data)\n",
    "    words = tfidf_vect_fit.get_feature_names()\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()  \n",
    "tfidf_vect_fit = tfidf_vect.fit(df_clean['text'])\n",
    "X_tfidf = vectorize(df_clean[\"text\"], tfidf_vect_fit)\n",
    "y = df[\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelly\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11570157, 12676700)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "tokenized_tweet = df_clean.text.apply(lambda x: x.split())  # tokenizing\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "    tokenized_tweet,\n",
    "    vector_size=200,  # desired no. of features/independent variables\n",
    "    window=5,  # context window size\n",
    "    min_count=2,  # Ignores all words with total frequency lower than 2.\n",
    "    sg=1,  # 1 for skip-gram model\n",
    "    hs=0,\n",
    "    negative=10,  # for negative sampling\n",
    "    workers=32,  # no.of cores\n",
    "    seed=34\n",
    ")\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples=len(\n",
    "    df_clean.text), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delux', 0.5371038913726807),\n",
       " ('rally', 0.5282623171806335),\n",
       " ('smashfries', 0.5274894833564758),\n",
       " ('unremarkable', 0.5255366563796997),\n",
       " ('charburger', 0.5223527550697327),\n",
       " ('shoestring', 0.5182007551193237),\n",
       " ('patty', 0.5118928551673889),\n",
       " ('texan', 0.5118187665939331),\n",
       " ('watercress', 0.5062969923019409),\n",
       " ('blu', 0.4975314140319824)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"burger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i, :] = word_vector(tokenized_tweet[i], 200)\n",
    "X_word2vec_df = pd.DataFrame(wordvec_arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17130)\t2\n",
      "  (0, 28506)\t1\n",
      "  (0, 26448)\t1\n",
      "  (0, 16151)\t1\n",
      "  (0, 12364)\t2\n",
      "  (0, 17983)\t3\n",
      "  (0, 3059)\t1\n",
      "  (0, 10407)\t1\n",
      "  (0, 3643)\t1\n",
      "  (0, 1453)\t8\n",
      "  (0, 13790)\t9\n",
      "  (0, 28165)\t8\n",
      "  (0, 9339)\t3\n",
      "  (0, 26035)\t10\n",
      "  (0, 28264)\t1\n",
      "  (0, 19025)\t1\n",
      "  (0, 28411)\t1\n",
      "  (0, 15613)\t2\n",
      "  (0, 23477)\t1\n",
      "  (0, 18269)\t1\n",
      "  (0, 18342)\t1\n",
      "  (0, 26049)\t4\n",
      "  (0, 11710)\t1\n",
      "  (0, 1429)\t1\n",
      "  (0, 668)\t1\n",
      "  :\t:\n",
      "  (9999, 9776)\t1\n",
      "  (9999, 9700)\t1\n",
      "  (9999, 18088)\t1\n",
      "  (9999, 26468)\t1\n",
      "  (9999, 5113)\t1\n",
      "  (9999, 2196)\t1\n",
      "  (9999, 281)\t1\n",
      "  (9999, 16519)\t1\n",
      "  (9999, 15182)\t2\n",
      "  (9999, 13793)\t1\n",
      "  (9999, 21936)\t1\n",
      "  (9999, 13811)\t1\n",
      "  (9999, 24585)\t1\n",
      "  (9999, 15277)\t1\n",
      "  (9999, 10089)\t1\n",
      "  (9999, 3329)\t1\n",
      "  (9999, 16570)\t1\n",
      "  (9999, 291)\t1\n",
      "  (9999, 24311)\t1\n",
      "  (9999, 3865)\t1\n",
      "  (9999, 18711)\t1\n",
      "  (9999, 863)\t1\n",
      "  (9999, 14632)\t1\n",
      "  (9999, 8386)\t1\n",
      "  (9999, 1303)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "tokenizer = Tokenizer(num_words = 3800)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X_keras = tokenizer.texts_to_sequences(df['text'])\n",
    "X_keras = sequence.pad_sequences(X_keras, maxlen=380)\n",
    "y = df[\"stars\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "def K_fold_CV(k, X, y):\n",
    "    #設定subset size 即data長度/k\n",
    "    #設定Accuracy初始值\n",
    "    X_train_folds = np.array_split(X, k)\n",
    "    y_train_folds = np.array_split(y, k)\n",
    "    Accuracy = 0\n",
    "    Precision = 0\n",
    "    Recall = 0\n",
    "    F1score = 0\n",
    "    for i in range(k):\n",
    "        # 設定testing set與training set的資料起始點與結束點\n",
    "        # 例如資料有100筆，testing set在本次iteration取第1到25筆，則training set為第26到100筆；下次testing set為26~50，training set為1~25 & 51~100\n",
    "        X_test = X_train_folds[i]\n",
    "        y_test = y_train_folds[i]\n",
    "        X_train_k = np.concatenate(X_train_folds[:i] + X_train_folds[i+1:])\n",
    "        y_train_k = np.concatenate(y_train_folds[:i] + y_train_folds[i+1:])\n",
    "        # 利用training set建立模型\n",
    "        rf.fit(X_train_k, y_train_k)\n",
    "        y_val_pred = rf.predict(X_test)\n",
    "        # testing set計算出Accuracy累加\n",
    "        eachAccuracy = accuracy_score(y_test, y_val_pred)\n",
    "        Accuracy += eachAccuracy\n",
    "        eachPrecision = precision_score(y_test,y_val_pred)\n",
    "        Precision += eachPrecision\n",
    "        eachRecall = recall_score(y_test, y_val_pred)\n",
    "        Recall += eachRecall\n",
    "        eachF1score = f1_score(y_test,y_val_pred)\n",
    "        F1score += eachF1score\n",
    "    return Accuracy/k, F1score/k, Precision/k, Recall/k\n",
    "\n",
    "\n",
    "\n",
    "accuracy_idf, f1score_idf, precision_idf, recall_idf = K_fold_CV(4,X_tfidf, y)\n",
    "print(\"TF_IDF :4-fold_CV accuracy: {}\".format(accuracy_idf))\n",
    "print(\"TF_IDF :4-fold_CV f1score: {}\".format(f1score_idf))\n",
    "print(\"TF_IDF :4-fold_CV precision: {}\".format(precision_idf))\n",
    "print(\"TF_IDF :4-fold_CV recall: {}\".format(recall_idf))\n",
    "# accuracy_word2vec,f1score_word2vec, precision_word2vec, recall_word2vec = K_fold_CV(4,X_word2vec_df, y)\n",
    "# print(\"word2Vec : 4-fold_CV accuracy: {}\".format(accuracy_word2vec))\n",
    "# print(\"word2Vec : 4-fold_CV f1score: {}\".format(f1score_word2vec))\n",
    "# print(\"word2Vec : 4-fold_CV precision: {}\".format(precision_word2vec))\n",
    "# print(\"word2Vec : 4-fold_CV recall: {}\".format(recall_word2vec))\n",
    "# print(\"word2Vec : 4-fold_CV accuracy: {}\".format(accuracy_keras))/\n",
    "# accuracy_keras = K_fold_CV(4,X_keras,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f2899a958f546442325085d57e097c29c1f10e1ff35ce384505b5cedb62c39f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
